{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawling NBA teams and players\n",
    "\n",
    "## 1. Introduction\n",
    "I decided to implement a crawler that extracts information about NBA teams and players. The main URL is 'http://www.espn.com/nba/players', belonging to ESPN, an american company focused on broadcasting sports. I decided to crawl from this site because I had some issues (described later) with the official NBA site.\n",
    "\n",
    "## 2. Issues with crawling and description of robots.txt\n",
    "The main issues with crawling could be:\n",
    "- Technical limitations: if content is dynamically generated with JavaScript some search engine bots may not be able to crawl the content.\n",
    "- Server overload: the website's servers can become slow or unresponsive if the load or the requests' frequency is too high.\n",
    "- Duplicate content: If bots crawl multiple versions of the same page you can experience duplicate content issues.\n",
    "- Security issues: a bot can get blocked if it violates security measures of a website.\n",
    "- Legal issues: crawling private or copyrighted information may be illegal or unethical.\n",
    "\n",
    "The robots.txt file of this site can be seen here: http://www.espn.com/robots.txt\n",
    "It sets some rules valid for all user-agents. Firstly there are a lot of \"Disallow\" lines that specify the URLs that should not be crawled or indexed (scoreboard,week,year,admin,boxscore,etc...). Then there are a few \"Allow\" lines that, on the other hand, specify the URLs that are allowed to be crawled and indexed, even if they are under a disallowed parent directory. In this particular robots.txt the \"Allow\" lines specify the URLs that are allowed to be crawled on the AMP version of the website. Finally there's the \"Sitemap\" line that specifies the location of the website's sitemap, which is a file that lists all the pages on the website that are available for crawling. In this case, the sitemap is located at https://www.espn.com/sitemap.xml. This line tells web crawlers to access and use this sitemap file to discover all the pages on the website that they are allowed to crawl.\n",
    "\n",
    "## 3. Input / Output\n",
    "The input is the start url and the output are two json files: one contains informations about each NBA team and the other informations about each NBA player. The output data are contained in the files 'teams.json' and 'players.json'.\n",
    "\n",
    "## 4. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required packages\n",
    "import json\n",
    "import time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definition of some useful functions\n",
    "\n",
    "# This function helps me dealing with incomplete values (explained later in the 'issues' section)\n",
    "def check_null_other(x):\n",
    "    \n",
    "    res = x.findAll('div')\n",
    "    if res == None:\n",
    "        return 'NONE'\n",
    "    else:\n",
    "        return res[-1].getText()\n",
    "\n",
    "\n",
    "# This function helps me dealing with incomplete values (explained later in the 'issues' section)\n",
    "def check_null_stats(x):\n",
    "    \n",
    "    res = x.find('div', class_='StatBlockInner__Value')\n",
    "    if res == None:\n",
    "        return 'NONE'\n",
    "    else:\n",
    "        return res.getText()\n",
    "\n",
    "\n",
    "# Function to get a player informations\n",
    "def crawl_player_info(page, fp):\n",
    "    \n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # Name and surname\n",
    "    name_surname = soup.find('h1', class_='PlayerHeader__Name')\n",
    "    if name_surname == None:\n",
    "        name_surname = ['NONE','NONE']\n",
    "    else:\n",
    "        name_surname = name_surname.findAll('span')\n",
    "        if name_surname == None:\n",
    "            name_surname = ['NONE','NONE']\n",
    "        else:\n",
    "            if(len(name_surname)==0):\n",
    "                name_surname = ['NONE','NONE']\n",
    "    \n",
    "    # General info (postioned under the name): team name, jersey number and role\n",
    "    info_s = soup.find('ul', class_='PlayerHeader__Team_Info')\n",
    "    if info_s == None:\n",
    "        team = 'NONE'\n",
    "        jersey_num = 'NONE'\n",
    "        position = 'NONE'\n",
    "    else:\n",
    "        info_s = info_s.findAll('li')\n",
    "        if info_s == None:\n",
    "            team = 'NONE'\n",
    "            jersey_num = 'NONE'\n",
    "            position = 'NONE'\n",
    "        else:\n",
    "            if(info_s == None or len(info_s) < 3):\n",
    "                team = 'NONE'\n",
    "                jersey_num = 'NONE'\n",
    "                position = 'NONE'\n",
    "            else:\n",
    "                team = info_s[0].getText()\n",
    "                jersey_num = info_s[1].getText()\n",
    "                position = info_s[2].getText()\n",
    "    \n",
    "    # Statistics: points per game, rebounds per game, \n",
    "    # assists per game, field goal percentage\n",
    "    stats = soup.find('ul', class_='StatBlock__Content')\n",
    "    if stats == None:\n",
    "        stats = ['NONE','NONE','NONE','NONE']\n",
    "    else:\n",
    "        stats = stats.findAll('li')\n",
    "        if stats == None:\n",
    "            stats = ['NONE','NONE','NONE','NONE']\n",
    "        else:\n",
    "            stats = [check_null_stats(s) for s in stats]\n",
    "            if stats == None:\n",
    "                stats = ['NONE','NONE','NONE','NONE']\n",
    "    \n",
    "    # Other info: height, weight, birthdate/age, college,  \n",
    "    # which draft he was selected in, status of his career\n",
    "    other = soup.find('ul', class_='PlayerHeader__Bio_List')\n",
    "    if other == None:\n",
    "        hw = 'NONE'\n",
    "        birthdate = 'NONE'\n",
    "        college = 'NONE'\n",
    "        draft = 'NONE'\n",
    "        status = 'NONE'\n",
    "    else:\n",
    "        other = other.findAll('li')\n",
    "        if other == None or len(other) < 5:\n",
    "            hw = 'NONE'\n",
    "            birthdate = 'NONE'\n",
    "            college = 'NONE'\n",
    "            draft = 'NONE'\n",
    "            status = 'NONE'\n",
    "        else:\n",
    "            hw = check_null_other(other[0]).split(',')\n",
    "            birthdate = check_null_other(other[1])\n",
    "            college = check_null_other(other[2])\n",
    "            draft = check_null_other(other[3])\n",
    "            status = check_null_other(other[4])\n",
    "    \n",
    "    player = {\n",
    "        'name': name_surname[0].getText(),\n",
    "        'surname': name_surname[1].getText(),\n",
    "        'team': team,\n",
    "        'jersey number': jersey_num,\n",
    "        'role': position,\n",
    "        'ppg': stats[0],\n",
    "        'rpg': stats[1],\n",
    "        'apg': stats[2],\n",
    "        'fg%': stats[3] + ' %',\n",
    "        'height': hw[0],\n",
    "        'weight': hw[1].strip(),\n",
    "        'birthdate (age)': birthdate,\n",
    "        'draft': draft,\n",
    "        'college': college,\n",
    "        'status': status,   \n",
    "    }\n",
    "    json_player = json.dumps(player, indent=2)\n",
    "    fp.write(json_player)\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "# Function to get a team information and team's players links\n",
    "def crawl_team_roster(page, ft, fp):\n",
    "    \n",
    "    soup = BeautifulSoup(page.content, 'html.parser')\n",
    "    \n",
    "    # Name of the team (city and name)\n",
    "    name = soup.findAll('span', class_='db')\n",
    "    if name == None:\n",
    "        name = 'NONE'\n",
    "    else:\n",
    "        name = name[0].getText() + ' ' + name[1].getText()\n",
    "    \n",
    "    # Record\n",
    "    record = soup.find('ul', class_='list flex ClubhouseHeader__Record n8 ml4')\n",
    "    if record == None:\n",
    "        position = 'NONE'\n",
    "        record = 'NONE'\n",
    "    else:\n",
    "        record = record.findAll('li')\n",
    "        if record == None:\n",
    "            position = 'NONE'\n",
    "            record = 'NONE' \n",
    "        else: \n",
    "            position = record[1].getText()\n",
    "            record = record[0].getText()\n",
    "    \n",
    "    # Write in json file\n",
    "    team = {\n",
    "        'name': name,\n",
    "        'record': record,\n",
    "        'postion': position,\n",
    "    }\n",
    "    json_team = json.dumps(team, indent=2)\n",
    "    ft.write(json_team)\n",
    "    \n",
    "    # Now parse all team's players information\n",
    "    crawled = []\n",
    "    links = soup.findAll('a', class_='AnchorLink')\n",
    "    if links == None:\n",
    "        print(\"Players' links not found, try again!!\")\n",
    "        return\n",
    "    else:\n",
    "        links = [l.get('href') for l in links if 'player/_/' in l.get('href')][0::2]\n",
    "    \n",
    "    for link in links:\n",
    "        if link not in crawled:\n",
    "            page = requests.get(link)\n",
    "            if page.status_code == 200:\n",
    "                crawl_player_info(page, fp)\n",
    "                crawled.append(link)\n",
    "            else:\n",
    "                print('Code ' + str(page.status_code) + ' for link: ' + link)\n",
    "            time.sleep(1)\n",
    "    \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup of variables\n",
    "url = 'http://www.espn.com/nba/players'\n",
    "start = requests.get(url)\n",
    "soup = BeautifulSoup(start.content, 'html.parser')\n",
    "links = []\n",
    "visited = []\n",
    "\n",
    "# json files where results will be written \n",
    "# !!! directories have to be changed for the faculty gitlab !!!\n",
    "fp = open(\"players1.json\", \"w\")\n",
    "ft = open(\"teams1.json\", \"w\") \n",
    "\n",
    "# Links of each team\n",
    "links = [l.get('href') for l in soup.findAll('a') if 'name' in l.get('href')]\n",
    "links = [l.replace('_', 'roster/_') for l in links]\n",
    "\n",
    "for link in links:\n",
    "    if link not in visited:\n",
    "        page = requests.get(link)\n",
    "        if page.status_code == 200:\n",
    "            crawl_team_roster(page, ft, fp)\n",
    "            visited.append(link)\n",
    "        else:\n",
    "            print('Code ' + str(page.status_code) + ' for link: ' + link)\n",
    "        time.sleep(1)\n",
    "    \n",
    "ft.close()\n",
    "fp.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Issues experienced and possible extensions/improvements\n",
    "\n",
    "At first I decided to crawl these informations from the official NBA site (https://www.nba.com/players) but I experienced three main problems:\n",
    "sometimes the server was sending empty pages and even reducing drastically the frequency of requests (1 each 10 seconds) didn't solve the problem; the second was that the page shows just the first 50 players and if you want to see all of them you have to select the filter 'all' from a dropdown menu. The problem here was that clicking on the filter doesn't change the url so I think the content is generated dynamically and I don't know how to deal with it. The third one was that sometimes I was getting a 'TimeoutError' and I could'nt find the source of that error. With the ESPN site I didn't experienced many issues, except that the elements of the site aren't always given a specific class/id forcing me to do nested call of the 'find' method. The only important issue I faced is that some players have incomplete data (for example a player that is free agent doesn't have a jersey number at the moment, or a european player that didn't go to an american college doesn't have this information) and for this reason I had to check every single value and substitute it with 'NONE' if missing. Anyway, there are hundreds of players with 15 attributes each and there are in total 36 'NONE' values, so it's barely noticeable.\n",
    "\n",
    "To conclude, a possibile extension could be crawling the players' data for all past seasons and analyzing their performance over the year or understanding which team changed the most players during the years. A possible improvement could be implementing a crawler able to deal with dynamically generated content."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
