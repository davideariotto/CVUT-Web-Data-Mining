{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Indexing + Document Retrieval\n",
    "## Libraries and useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "import scipy.sparse\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading data\n",
    "def get_queries():\n",
    "    return [open(\"./cranfield/q/\" + str(q) + \".txt\").read() for q in range(1, 226)]\n",
    "\n",
    "def get_documents():\n",
    "    return [open(\"./cranfield/d/\" + str(d + 1) + \".txt\").read() for d in range(1400)]\n",
    "\n",
    "def get_relevant_documents(query_id):\n",
    "    res = []\n",
    "    with open(\"./cranfield/r/{}.txt\".format(query_id)) as f:\n",
    "        for line in f.readlines():\n",
    "            res.append(int(line))\n",
    "    return res\n",
    "\n",
    "def get_data(query):\n",
    "    data = list(get_documents())\n",
    "    data.append(query)\n",
    "    return data\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# distance measures\n",
    "def euclidean(data, length):\n",
    "    sim = np.array(euclidean_distances(data[length], data[0:length])[0])\n",
    "    return sim.argsort()+1\n",
    "\n",
    "def cosine(data, length):\n",
    "    sim = np.array(cosine_similarity(data[length], data[0:length])[0])\n",
    "    return sim.argsort()[::-1]+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# score functions\n",
    "def calculate_precision(retrieved_documents, relevant_documents):\n",
    "    relevant_count = 0\n",
    "    for doc in retrieved_documents:\n",
    "        if doc in relevant_documents:\n",
    "            relevant_count += 1\n",
    "\n",
    "    return relevant_count / len(retrieved_documents)\n",
    "\n",
    "def calculate_recall(retrieved_documents, relevant_documents):\n",
    "    relevant_count = 0\n",
    "    for doc in retrieved_documents:\n",
    "        if doc in relevant_documents:\n",
    "            relevant_count += 1\n",
    "\n",
    "    return relevant_count / len(relevant_documents)\n",
    "\n",
    "def calculate_fmeasure(precision, recall):\n",
    "    if precision == 0 and recall == 0:\n",
    "        return 0\n",
    "\n",
    "    return 2 * (precision * recall) / (precision + recall)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of Binary, Term frequency and TFIDF weighting schemas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# weighting schemas\n",
    "def binary(query):\n",
    "    data = get_data(query)\n",
    "\n",
    "    vectorizer = CountVectorizer(binary=True)\n",
    "    count_array = vectorizer.fit_transform(data)\n",
    "    \n",
    "    return euclidean(count_array, len(data)-1), cosine(count_array, len(data)-1)\n",
    "\n",
    "def term_frequency(query):\n",
    "    data = get_data(query)\n",
    "    \n",
    "    tf_vectorizer = CountVectorizer()\n",
    "    tf_matrix = tf_vectorizer.fit_transform(data)\n",
    "    \n",
    "    sum_x = tf_matrix.sum(1)\n",
    "    new_tf_matrix = tf_matrix.multiply(1 / sum_x)\n",
    "    new_tf_matrix = scipy.sparse.csc_matrix(new_tf_matrix)\n",
    "    \n",
    "    return euclidean(new_tf_matrix, len(data)-1), cosine(new_tf_matrix, len(data)-1)\n",
    "\n",
    "def tfidf(query):\n",
    "    data = get_data(query)\n",
    "    \n",
    "    tfidf_vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(data)\n",
    "    \n",
    "    return euclidean(tfidf_matrix, len(data)-1), cosine(tfidf_matrix, len(data)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/78/m6cr7x9n6ssgx5dprvlql0bm0000gn/T/ipykernel_7069/1842453120.py:17: RuntimeWarning: divide by zero encountered in divide\n",
      "  new_tf_matrix = tf_matrix.multiply(1 / sum_x)\n"
     ]
    }
   ],
   "source": [
    "# global variables\n",
    "results = []\n",
    "N = 15 # limit to top N\n",
    "\n",
    "\n",
    "# set up csv file\n",
    "header = ['Euclidean binary precision',\n",
    "'Euclidean binary recall',\n",
    "'Euclidean binary F-measure',\n",
    "'Euclidean Term Frequency precision',\n",
    "'Euclidean Term Frequency recall',\n",
    "'Euclidean Term Frequency F-measure',\n",
    "'Euclidean TF-IDF precision',\n",
    "'Euclidean TF-IDF recall',\n",
    "'Euclidean TF-IDF F-measure',\n",
    "'Cosine binary precision',\n",
    "'Cosine binary recall',\n",
    "'Cosine binary F-measure',\n",
    "'Cosine Term Frequency precision',\n",
    "'Cosine Term Frequency recall',\n",
    "'Cosine Term Frequency F-measure',\n",
    "'Cosine TF-IDF precision',\n",
    "'Cosine TF-IDF recall',\n",
    "'Cosine TF-IDF F-measure', '\\n']\n",
    "\n",
    "with open('standard_output.csv', 'w') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerow(header)\n",
    "    csvFile.close()   \n",
    "\n",
    "# perform task for each query\n",
    "query_id = 1   \n",
    "queries = get_queries()\n",
    "for query in queries:\n",
    "    binary_result = binary(query)\n",
    "    tf_result = term_frequency(query)\n",
    "    tfidf_result = tfidf(query)\n",
    "\n",
    "    relevant_docs = get_relevant_documents(query_id)\n",
    "\n",
    "    e_binary_precision = calculate_precision(binary_result[0][:N], relevant_docs)\n",
    "    e_binary_recall = calculate_recall(binary_result[0][:N], relevant_docs)\n",
    "    e_binary_fmeasure = calculate_fmeasure(e_binary_precision, e_binary_recall)\n",
    "    e_tf_precision = calculate_precision(tf_result[0][:N], relevant_docs)\n",
    "    e_tf_recall = calculate_recall(tf_result[0][:N], relevant_docs)\n",
    "    e_tf_fmeasure = calculate_fmeasure(e_tf_precision, e_tf_recall)\n",
    "    e_tfidf_precision = calculate_precision(tfidf_result[0][:N], relevant_docs)\n",
    "    e_tfidf_recall = calculate_recall(tfidf_result[0][:N], relevant_docs)\n",
    "    e_tfidf_fmeasure = calculate_fmeasure(e_tfidf_precision, e_tfidf_recall)\n",
    "    \n",
    "    c_binary_precision = calculate_precision(binary_result[1][:N], relevant_docs)\n",
    "    c_binary_recall = calculate_recall(binary_result[1][:N], relevant_docs)\n",
    "    c_binary_fmeasure = calculate_fmeasure(c_binary_precision, c_binary_recall)\n",
    "    c_tf_precision = calculate_precision(tf_result[1][:N], relevant_docs)\n",
    "    c_tf_recall = calculate_recall(tf_result[1][:N], relevant_docs)\n",
    "    c_tf_fmeasure = calculate_fmeasure(c_tf_precision, c_tf_recall)\n",
    "    c_tfidf_precision = calculate_precision(tfidf_result[1][:N], relevant_docs)\n",
    "    c_tfidf_recall = calculate_recall(tfidf_result[1][:N], relevant_docs)\n",
    "    c_tfidf_fmeasure = calculate_fmeasure(c_tfidf_precision, c_tfidf_recall)\n",
    "    \n",
    "    res = [e_binary_precision, e_binary_recall, e_binary_fmeasure, e_tf_precision, e_tf_recall, e_tf_fmeasure, e_tfidf_precision, e_tfidf_recall,\n",
    "                e_tfidf_fmeasure, c_binary_precision, c_binary_recall,  c_binary_fmeasure,  c_tf_precision, c_tf_recall, c_tf_fmeasure, \n",
    "                c_tfidf_precision, c_tfidf_recall, c_tfidf_fmeasure]\n",
    "    \n",
    "    with open('standard_output.csv', 'a') as csvFile:\n",
    "        writer = csv.writer(csvFile)\n",
    "        writer.writerow([\"{:.3f}\".format(float(r)) for r in res]+['\\n'])\n",
    "        csvFile.close()\n",
    "    \n",
    "    results.append(res)\n",
    "    \n",
    "    query_id += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average scores:\n",
      "--------------------------------------------------------------------------------\n",
      "Euclidean binary precision = 0.015\n",
      "Euclidean binary recall = 0.031\n",
      "Euclidean binary F-measure = 0.018\n",
      "Euclidean Term Frequency precision = 0.090\n",
      "Euclidean Term Frequency recall = 0.182\n",
      "Euclidean Term Frequency F-measure = 0.113\n",
      "Euclidean TF-IDF precision = 0.179\n",
      "Euclidean TF-IDF recall = 0.378\n",
      "Euclidean TF-IDF F-measure = 0.227\n",
      "Cosine binary precision = 0.118\n",
      "Cosine binary recall = 0.260\n",
      "Cosine binary F-measure = 0.152\n",
      "Cosine Term Frequency precision = 0.106\n",
      "Cosine Term Frequency recall = 0.223\n",
      "Cosine Term Frequency F-measure = 0.134\n",
      "Cosine TF-IDF precision = 0.188\n",
      "Cosine TF-IDF recall = 0.395\n",
      "Cosine TF-IDF F-measure = 0.238\n"
     ]
    }
   ],
   "source": [
    "# Compute average\n",
    "sum_values = [0] * 19\n",
    "for res in results:\n",
    "    for i in range(len(res)):\n",
    "        sum_values[i] += res[i]\n",
    "        \n",
    "print(\"Average scores:\")\n",
    "print(\"-\" * 80)\n",
    "for i in range(len(header)-1):\n",
    "    print(header[i] + \" = {:.3f}\".format(sum_values[i] / len(queries)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Hugging Face sentence similarity model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query number 225\r"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer('paraphrase-MiniLM-L6-v2')\n",
    "huggingface_results = []\n",
    "\n",
    "# set up csv file\n",
    "header = ['Euclidean precision', 'Euclidean recall', 'Euclidean F-measure', 'Cosine precision', 'Cosine recall', 'Cosine F-measure', '\\n']\n",
    "with open('huggingface_output.csv', 'w') as csvFile:\n",
    "    writer = csv.writer(csvFile)\n",
    "    writer.writerow(header)\n",
    "    csvFile.close()   \n",
    "\n",
    "query_id = 1\n",
    "for query in queries:\n",
    "    print(\"Query number \" + str(query_id), end=\"\\r\")\n",
    "    relevant_docs = get_relevant_documents(query_id)\n",
    "    \n",
    "    # Compute embeddings\n",
    "    sentences = get_data(query)\n",
    "    sentence_embeddings = model.encode(sentences)\n",
    "    similarities = []\n",
    "    doc_id = 1\n",
    "    for s in sentence_embeddings[:-1]:\n",
    "        # Compute differences\n",
    "        # 'similarities' is a list of tuples. Each tuple contains: euclidean distance, cosine similarity, document id\n",
    "        similarities.append((euclidean_distances(s.reshape(1, -1), sentence_embeddings[-1].reshape(1, -1)),\n",
    "                             cosine_similarity(s.reshape(1, -1), sentence_embeddings[-1].reshape(1, -1)),\n",
    "                             doc_id))   \n",
    "        doc_id += 1\n",
    "    \n",
    "    # Order and select N most similar documents\n",
    "    euclidean_res = sorted(similarities, key=lambda x: x[0], reverse=True)\n",
    "    cosine_res = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "    euclidean_retrieved_docs = [d[2] for d in euclidean_res[:N]]\n",
    "    cosine_retrieved_docs = [d[2] for d in cosine_res[:N]]\n",
    "    \n",
    "    # Compute scores\n",
    "    e_p = calculate_precision(euclidean_retrieved_docs, relevant_docs)\n",
    "    e_r = calculate_recall(euclidean_retrieved_docs, relevant_docs)\n",
    "    e_f = calculate_fmeasure(e_p, e_r)\n",
    "    c_p = calculate_precision(cosine_retrieved_docs, relevant_docs)\n",
    "    c_r = calculate_recall(cosine_retrieved_docs, relevant_docs)\n",
    "    c_f = calculate_fmeasure(c_p, c_r)\n",
    "    \n",
    "    # Save results\n",
    "    huggingface_results.append([e_p, e_r, e_f, c_p, c_r, c_f])\n",
    "    with open('huggingface_output.csv', 'a') as csvFile:\n",
    "        writer = csv.writer(csvFile)\n",
    "        writer.writerow([\"{:.3f}\".format(float(r)) for r in [e_p, e_r, e_f, c_p, c_r, c_f]]+['\\n'])\n",
    "        csvFile.close()\n",
    "    \n",
    "    query_id += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average scores:\n",
      "--------------------------------------------------------------------------------\n",
      "Euclidean Precision = 0.001\n",
      "Euclidean Recall = 0.001\n",
      "Euclidean F-Measure = 0.001\n",
      "Cosine Precision = 0.196\n",
      "Cosine Recall = 0.421\n",
      "Cosine F-Measure = 0.249\n"
     ]
    }
   ],
   "source": [
    "# Compute average\n",
    "sum_values = [0] * 6\n",
    "for res in huggingface_results:\n",
    "    for i in range(len(res)):\n",
    "        sum_values[i] += res[i]\n",
    "\n",
    "text = [\"Euclidean Precision\", \"Euclidean Recall\", \"Euclidean F-Measure\", \"Cosine Precision\", \"Cosine Recall\", \"Cosine F-Measure\"]\n",
    "print(\"Average scores:\")\n",
    "print(\"-\" * 80)\n",
    "for t, r in zip(text, sum_values):\n",
    "    print(\"{} = {:.3f}\".format(t, r / len(queries)))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments on results\n",
    "\n",
    "#### Complete results can be found in the two csv files (standard_output.csv for the first implementation and huggingface_output.csv for the part using sentence similarity model from Hugging Face). The average scores can be found in the two png images (standard.png and huggingface.png). As explained during the lecture the cosine similarity measure performed way better for all the models, except for tfidf where the difference is barely noticeable. Comparing the three different 'base' models, tfidf perfomed much better than the prevoius two, both with euclidean distance and cosine similarity. We can also notice that with cosine similarity the difference between binary representation and term frequency is minimal, while with euclidean distance term frequency performed better than binary. Anyway, as I said before, tfidf perfomed better than both on them. Regarding the sentence similarity model from Hugging Face, it perfomed better than all of the previous models when we use cosine similarity, even if the difference with tfidf is negligible. On the other hand, results with euclidean distance were very poor. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues and possible extensions\n",
    "\n",
    "#### I didn't have particular issues during this homework, but I expected better results, especially with the Hugging Face model. It also took a lot of time to do the task (almost three hours) so I expected better scores. As an extension, I would look for better models or better ways to encode the documents in order to obtain better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
