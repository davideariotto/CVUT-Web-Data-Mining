{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems\n",
    "\n",
    "#### First I imported the necessary libraries and the data. I also implemented a couple objects and some utils functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import csv\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import normalize\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.neighbors import NearestNeighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Firstly let's create training and testing files\n",
    "with open('ml-latest-small/ratings.csv', 'r') as input_file:\n",
    "    # Create two output files for training and testing\n",
    "    with open('ml-latest-small/ratings_train.csv', 'w', newline='') as training_file, open('ml-latest-small/ratings_test.csv', 'w', newline='') as testing_file:\n",
    "        # Create CSV writers for the output files\n",
    "        training_writer = csv.writer(training_file)\n",
    "        testing_writer = csv.writer(testing_file)\n",
    "        \n",
    "        # Loop over the input file and split the rows into the training and testing files\n",
    "        for i, row in enumerate(csv.reader(input_file)):\n",
    "            if i % 2 == 0:\n",
    "                # Even rows go to the training file\n",
    "                training_writer.writerow(row)\n",
    "            else:\n",
    "                # Odd rows go to the testing file\n",
    "                testing_writer.writerow(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class User:\n",
    "    def __init__(self, user_id, genres_count):\n",
    "        super().__init__()\n",
    "        self.id = user_id\n",
    "        self.genre_ratings = np.zeros(genres_count, dtype=int)  # Ratings of genres by the user, normalized to (0,1)\n",
    "        self.ratings: Dict[int, float] = {}  # Star-rating of given movies\n",
    "        self.movies_rated = set()\n",
    "\n",
    "class Movie:\n",
    "    def __init__(self, movie_id, title, genres):\n",
    "        super().__init__()\n",
    "        self.id = movie_id\n",
    "        self.title = title\n",
    "        self.genres = genres\n",
    "        self.genres_vector = None\n",
    "\n",
    "    def set_genres(self, genres_str_int):\n",
    "        self.genres_vector = np.zeros(len(genres_str_int))\n",
    "        for genre in self.genres:\n",
    "            genre_id = genres_str_int[genre]\n",
    "            self.genres_vector[genre_id] = 1\n",
    "\n",
    "        self.genres_vector = self.genres_vector.reshape(1, -1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input part\n",
    "\n",
    "\n",
    "# import movies from the file. Output format is a tuple (dictionary of Movie objects, list of genres)\n",
    "def read_movies(path):\n",
    "    genres_set = set() # initially a set so I don't have to check for duplicates\n",
    "    movies = {}\n",
    "    inverted_movies = {}\n",
    "\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        #f.readline() # skip the header\n",
    "        f.readline()\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for row in reader:\n",
    "            # print(\"Row: {}\".format(row))\n",
    "            movie_id = int(row[0])\n",
    "            title = row[1]\n",
    "            genres = [genre.strip() for genre in row[2].split('|') if genre.strip() != '(no genres listed)']\n",
    "            movie = Movie(movie_id, title, genres)\n",
    "            movies[movie_id] = movie\n",
    "            inverted_movies[title] = movie_id\n",
    "            genres_set = genres_set.union(set(genres))\n",
    "\n",
    "    return movies, sorted(list(genres_set)), inverted_movies\n",
    "    \n",
    "# import users ratings from the file. Output format is a dictionary of User objects (key is the id)\n",
    "def read_ratings(path, genre_str_id, movies):\n",
    "    users = {}\n",
    "    with open(path, encoding=\"utf-8\") as f:\n",
    "        f.readline() # skip the header\n",
    "        f.readline()\n",
    "        reader = csv.reader(f, delimiter=',')\n",
    "        for i, row in enumerate(reader):\n",
    "            user_id = int(row[0])\n",
    "            movie_id = int(row[1])\n",
    "            rating = float(row[2])\n",
    "            # create new user if necessary\n",
    "            if user_id not in users:\n",
    "                users[user_id] = User(user_id, len(genre_str_id))\n",
    "            # add rating if user already present\n",
    "            users[user_id].movies_rated.add(movie_id)\n",
    "            users[user_id].ratings[movie_id] = rating\n",
    "            # +1 to the genre field of the user profile if rating is >= 2.5\n",
    "            for movie_genre in movies[movie_id].genres:\n",
    "                if rating >= 2.5:\n",
    "                    users[user_id].genre_ratings[genre_str_id[movie_genre]] += 1\n",
    "\n",
    "    # Normalize scores\n",
    "    for user in users.values():\n",
    "        user.genre_ratings = user.genre_ratings / 5\n",
    "    \n",
    "    return users\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation measures\n",
    "def compute_recall(recommended_movies, rated_movies):\n",
    "    count = 0\n",
    "    for movie_id in recommended_movies:\n",
    "        if movie_id in rated_movies:\n",
    "            count += 1\n",
    "\n",
    "    return count / len(rated_movies)\n",
    "\n",
    "\n",
    "def compute_precision(recommended_movies, rated_movies):\n",
    "    count = 0\n",
    "    for movie_id in recommended_movies:\n",
    "        if movie_id in rated_movies:\n",
    "            count += 1\n",
    "\n",
    "    return count / len(recommended_movies)\n",
    "\n",
    "\n",
    "def compute_fmeasure(precision, recall):\n",
    "    if precision == 0 and recall == 0:\n",
    "        return 0\n",
    "\n",
    "    return 2 * (precision * recall) / (precision + recall)\n",
    "\n",
    "def evaluate_results(recommended_movies, testing_user):\n",
    "    \n",
    "    precision = compute_precision(recommended_movies, testing_user.movies_rated)\n",
    "    recall = compute_recall(recommended_movies, testing_user.movies_rated)\n",
    "    fmeasure = compute_fmeasure(precision, recall)\n",
    "\n",
    "    print(\"Statistics about recommendation for user {}\".format(testing_user.id))\n",
    "    print(\"Precision = {:.3f}\".format(precision))\n",
    "    print(\"Recall = {:.3f}\".format(recall))\n",
    "    print(\"F-Measure = {:.3f}\".format(fmeasure))\n",
    "    \n",
    "    return (precision, recall, fmeasure)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implementation of content based, collaborative filtering based and hybrid recommender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CONTENT BASED RECOMMENDER\n",
    "def recommend_content_based(movies, users, user_id, N):\n",
    "    similarities = {}\n",
    "\n",
    "    # focus only on movies that has not been rated yet by the user\n",
    "    non_rated_movies = [movie for movie in movies.values() if movie.id not in users[user_id].movies_rated]\n",
    "    # compute similarity\n",
    "    for movie in non_rated_movies:\n",
    "        similarities[movie.id] = cosine_similarity(users[user_id].genre_ratings.reshape(1, -1), movie.genres_vector)[0][0]\n",
    "\n",
    "    # Sort similarities\n",
    "    sorted_similarities = sorted(similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "    \n",
    "    return sorted_similarities[:N]\n",
    "    \n",
    "    \n",
    "# COLLABORATIVE FILTERING BASED RECOMMENDER\n",
    "def recommend_collaborative_based(users, user_id, N, Nu):\n",
    "    this_user = users[user_id]\n",
    "\n",
    "    # compute similarities with other users\n",
    "    similarities = {}\n",
    "    for user in users.values():\n",
    "        if user_id != user.id:\n",
    "            similarities[user.id] = cosine_similarity(this_user.genre_ratings.reshape(1, -1), user.genre_ratings.reshape(1, -1))[0][0]\n",
    "\n",
    "    # Get top Nu most similar users\n",
    "    sorted_similar_users = sorted(similarities.items(), key=lambda i: i[1], reverse=True)[:Nu]\n",
    "\n",
    "    # Get movies to recommend (their rating is the average of the rating given by the similar users)\n",
    "    temp_ratings = {}  # {movie_id: list of ratings}\n",
    "    for user, user_similarity in [(users[u_id], sim) for u_id, sim in sorted_similar_users]:   \n",
    "        for movie_id, movie_rating in user.ratings.items():  # Iterate through the Users' ratings\n",
    "            # Focus only on movies that the user hasn't rated\n",
    "            if movie_id not in this_user.movies_rated:    \n",
    "                if movie_id not in temp_ratings:\n",
    "                    temp_ratings[movie_id] = []\n",
    "                # Add rating (weighted to the similarity)\n",
    "                temp_ratings[movie_id].append(movie_rating * user_similarity)\n",
    "\n",
    "    # Calculate average\n",
    "    avg_ratings = {movie_id: sum(list_rating) / len(list_rating) for movie_id, list_rating in temp_ratings.items()}\n",
    "    \n",
    "    # Sort ratings\n",
    "    sorted_ratings = sorted(avg_ratings.items(), key=lambda r: r[1], reverse=True)\n",
    "    \n",
    "    # Normalize ratings\n",
    "    final_ratings = [(rating[0], (rating[1] / 5)) for rating in sorted_ratings]\n",
    "\n",
    "    # Select first N (they are sorted)\n",
    "    res = {}\n",
    "    for idx, (k, v) in enumerate(final_ratings):\n",
    "        if idx == N: break\n",
    "        res[k] = v\n",
    "    \n",
    "    return res\n",
    "    \n",
    "    \n",
    "# HYBRID BASED RECOMMENDER     \n",
    "def recommend_hybrid_based(movies, users, user_id, content_weight, collab_weight, N, Nu):\n",
    "    content_recommendations = recommend_content_based(movies, users, user_id, N)\n",
    "    content_recommendations = {t[0]: t[1] for t in content_recommendations}\n",
    "    collab_recommendations = recommend_collaborative_based(users, user_id, N, Nu)\n",
    "\n",
    "    # Get all movie ids avoiding duplicates thanks to 'set'\n",
    "    keys = list(set(list(content_recommendations.keys()) + list(collab_recommendations.keys())))\n",
    "\n",
    "    final_recommendations = {}\n",
    "    for key in keys:\n",
    "        # if movie is recommended by both recommenders, compute weighted score\n",
    "        if key in content_recommendations and key in collab_recommendations:\n",
    "            final_recommendations[key] = content_recommendations[key] * content_weight + collab_recommendations[key] * collab_weight\n",
    "         # else use the score from the only recommender that suggested it\n",
    "        elif key in content_recommendations and key not in collab_recommendations:\n",
    "            final_recommendations[key] = content_recommendations[key] * content_weight\n",
    "        else:\n",
    "            final_recommendations[key] = collab_recommendations[key] * collab_weight\n",
    "    \n",
    "    # Sort in descending order\n",
    "    sorted_final = sorted(final_recommendations.items(), key=lambda item: item[1], reverse=True)\n",
    "        \n",
    "    return sorted_final[:N]\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's perform the task with the first three recommenders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "N = 50      # number of results (number of recommendations)\n",
    "Nu = 20      # number of users to be kept in consideration in the collaborative filtering based recommender\n",
    "\n",
    "# Import files  \n",
    "movies, genres_list, inverted_movies = read_movies('ml-latest-small/movies.csv')\n",
    "\n",
    "# useful dictionaries\n",
    "genre_str_id = {genre: i for i, genre in enumerate(genres_list)}\n",
    "genre_id_str = {i: genre for i, genre in enumerate(genres_list)}\n",
    "\n",
    "for movie in movies.values():\n",
    "    movie.set_genres(genre_str_id)\n",
    "\n",
    "\n",
    "users = read_ratings('ml-latest-small/ratings.csv', genre_str_id, movies)\n",
    "\n",
    "#res = recommend_content_based(movies, users, 1, N)\n",
    "#for r in res:\n",
    "#    print(movies[r[0]].title)\n",
    "    \n",
    "#res = recommend_collaborative_based(users, 100, N, Nu)\n",
    "#for r in res.keys():\n",
    "#    print(movies[r].title)\n",
    "\n",
    "#res = recommend_hybrid_based(movies, users, 1, 0.3, 0.7, N, Nu)\n",
    "#for r in res:\n",
    "   #print(movies[r[0]].title)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "users_train = read_ratings('ml-latest-small/ratings_train.csv', genre_str_id, movies)\n",
    "users_test = read_ratings('ml-latest-small/ratings_test.csv', genre_str_id, movies)\n",
    "id_test = 11\n",
    "test_user = users_test[id_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CONTENT BASED RECOMMENDER\\n\")\n",
    "recommended_movies = recommend_content_based(movies, users_train, id_test, N)\n",
    "recommended_movies = [item[0] for item in recommended_movies]\n",
    "res = evaluate_results(recommended_movies, test_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"COLLABORATIVE FILTERING BASED RECOMMENDER\\n\")\n",
    "recommended_movies = recommend_collaborative_based(users_train, id_test, N, Nu)\n",
    "res = evaluate_results(list(recommended_movies.keys()), test_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"HYBRID RECOMMENDER\\n\")\n",
    "weights = [(0.1, 0.9), (0.3, 0.7), (0.7, 0.3), (0.9, 0.1)]\n",
    "for (content_weight, collab_weight) in weights:\n",
    "    print(\"-\"*70)\n",
    "    print(\"Content based weight: {}\".format(content_weight))\n",
    "    print(\"Collaborative filtering based weight: {}\\n\".format(collab_weight))\n",
    "    recommended_movies = recommend_hybrid_based(movies, users_train, id_test, content_weight, collab_weight, N, Nu)\n",
    "    recommended_movies = [item[0] for item in recommended_movies]\n",
    "    res = evaluate_results(recommended_movies, test_user)\n",
    "    print()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "#### The parameters I used are: number of movies to recommend = 50 and number of similar users to consider = 20. I ran the notebook for 5 randomly selected users. These parameters are also valid for the second part. The results for the content based recommender and the collaborative filtering based recommender can be seen in the 'results_base.csv' file. The results for the hybrid recommender can be seen in the 'results_hybrid.csv' file. Other than the results in the files I did more experiments and I've seen that the content based recommender worked better than the collaborative filtering one, even if they both had bad results overall. Or at least I was expecting better scores. Anyway one thing that I noticed is that content based and collaborative filtering recommended different movies and for this reason the hybrid recommender basically became a content based or a collaborative filtering one, depending on the weights. This is because of my implementation: if a movie is recommended by just one of the two recommenders its similarity gets scaled down by the corresponding weight factor, so basically if for example the weights are 0.1 and 0.9, movies recommended just by the model with a weight of 0.1 will never be selected."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-Nearest Neighbors based recommender\n",
    "\n",
    "#### The reason why I chose this model instead of Tensorflow recommenders can be found in the 'issues' section.\n",
    "#### Anyway, I first rearranged the data in order to have a table where each row is a user, each column a movie and the values are the ratings (zero if not rated yet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratings_dataset = pd.read_csv('ml-latest-small/ratings_train.csv', sep=',', header=0)\n",
    "del ratings_dataset['timestamp']\n",
    "ratings_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names = ['movie id', 'title', 'genres']\n",
    "movies_dataset = pd.read_csv('ml-latest-small/movies.csv', sep=',', header=0)\n",
    "movies_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres_dataset = items_dataset[['movieId', 'genres']]\n",
    "genres_list = ['Action', 'Adventure', 'Animation', 'Children', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Fantasy',\n",
    "                    'Film-Noir', 'Horror', 'Musical', 'Mystery', 'Romance', 'Sci-Fi','Thriller', 'War', 'Western', \n",
    "                    'IMAX', '(no genres listed)']\n",
    "for genre in genres_list:\n",
    "    genres_dataset[genre] = [0] * 9742 # (number of movies)\n",
    "\n",
    "for row in genres_dataset.iterrows():\n",
    "    index = row[0]\n",
    "    content = row[1]\n",
    "    film_genres = content['genres'].split('|')\n",
    "    for g in film_genres:\n",
    "        genres_dataset.iat[index, genres_dataset.columns.get_loc(g)] = 1\n",
    "\n",
    "del genres_dataset['genres']\n",
    "genres_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_movies = pd.merge(\n",
    "    movies_dataset,\n",
    "    genres_dataset,\n",
    "    how=\"inner\",\n",
    "    on='movieId',\n",
    ")\n",
    "del processed_movies['genres']\n",
    "del processed_movies['(no genres listed)']\n",
    "processed_movies.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_dataset = pd.merge(ratings_dataset, processed_movies, how='inner', on='movieId')\n",
    "merged_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "refined_dataset = merged_dataset.groupby(by=['userId','title'], as_index=False).agg({\"rating\":\"mean\"})\n",
    "user_to_movie_df = refined_dataset.pivot(\n",
    "    index='userId',\n",
    "    columns='title',\n",
    "    values='rating').fillna(0)\n",
    "\n",
    "user_to_movie_df.sample(n=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now let's perform the actual task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform matrix to scipy sparse matrix\n",
    "user_to_movie_sparse_df = csr_matrix(user_to_movie_df.values)\n",
    "\n",
    "# Define and fit the model\n",
    "knn_model = NearestNeighbors(metric='cosine', algorithm='brute')\n",
    "knn_model.fit(user_to_movie_sparse_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar_users(user, n):\n",
    "  knn_input = np.asarray([user_to_movie_df.values[user-1]]) \n",
    "  distances, indices = knn_model.kneighbors(knn_input, n_neighbors=n+1)\n",
    "  \n",
    "  print(\"Top\", n, \"most simlar users to the user\", user,\" are: \\n\")\n",
    "  for i in range(1,len(distances[0])):\n",
    "    print(\"{}. User {} --> {:.3f}\".format(i, indices[0][i]+1, distances[0][i]))\n",
    "  \n",
    "  return indices.flatten()[1:] + 1, distances.flatten()[1:]\n",
    "\n",
    "\n",
    "def filtered_movie_recommendations(n, mean_rating_list, refined_dataset, user_id):\n",
    "  \n",
    "  # Find the index of the last occurrence of 0 in mean_rating_list\n",
    "  zero_index = np.where(mean_rating_list == 0)[0][-1]\n",
    "\n",
    "  # Sort the mean_rating_list in descending order\n",
    "  sorted_indices = np.argsort(mean_rating_list)[::-1]\n",
    "\n",
    "  # Select only the indices before zero_index\n",
    "  filtered_indices = sorted_indices[:list(sorted_indices).index(zero_index)]\n",
    "\n",
    "  # Limit the number of indices to n\n",
    "  n_filtered_indices = min(len(filtered_indices), n)\n",
    "\n",
    "  # Get a list of movies watched by user_id\n",
    "  movies_watched = list(refined_dataset[refined_dataset['userId'] == user_id]['title'])\n",
    "\n",
    "  # Create a filtered list of movies based on the filtered_indices\n",
    "  filtered_movie_list = list(movies_list[filtered_indices[:n_filtered_indices]])\n",
    "\n",
    "  count = 0\n",
    "  final_movie_list = []\n",
    "  for m in filtered_movie_list:\n",
    "    if m not in movies_watched:\n",
    "      count += 1\n",
    "      final_movie_list.append(m)\n",
    "    if count == n:\n",
    "      break\n",
    "  \n",
    "  if count == 0:\n",
    "    print(\"No movies which are not seen by the input user and seen by similar users. Please increase the number of similar users.\")\n",
    "  elif count < n:\n",
    "    print(\"There aren't {} movies which are not seen by the input user and seen by similar users.\".format(n))\n",
    "    print(\"Anyway, I found {} to recommend. If you want {}, please increase the number of similar users to consider.\\nMovies:\".format(count, n))\n",
    "    for i, movie in zip(range(1,len(final_movie_list)+1), final_movie_list):\n",
    "      print(i,\". \",movie)\n",
    "  else:\n",
    "    for i, movie in zip(range(1,len(final_movie_list)+1), final_movie_list):\n",
    "      print(i,\". \",movie) \n",
    "  \n",
    "  return final_movie_list"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### At first we find the most similar users (and their similarity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "#id_test = 1\n",
    "num_users = 20\n",
    "num_movies = 50\n",
    "testing_user = users_test[id_test]\n",
    "\n",
    "\n",
    "similar_users_list, similarities_list = get_similar_users(user_id, num_users)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Then we get the recommended movies. The \"role\" of a similar user in the recommendation task is weighted to his similarity with the main user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_list = similarities_list / np.sum(similarities_list)\n",
    "\n",
    "# Movie ratings from similar users\n",
    "movie_ratings = user_to_movie_df.values[similar_users_list]\n",
    "movies_list = user_to_movie_df.columns\n",
    "\n",
    "# Broadcasting weight matrix to rating matrix so it is compatible for matrix operations\n",
    "weight_list = weight_list[:,np.newaxis] + np.zeros(len(movies_list))\n",
    "weighted_rating_matrix = weight_list * movie_ratings\n",
    "mean_rating_list = weighted_rating_matrix.sum(axis = 0)\n",
    "\n",
    "recommended_movies = filtered_movie_recommendations(num_movies, mean_rating_list, refined_dataset, user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recommended_movies = [inverted_movies[title] for title in recommended_movies]\n",
    "res = evaluate_results(recommended_movies, testing_user)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comments\n",
    "\n",
    "#### The results can be seen in the 'results_knn.csv' file. Using this model I got better results than the previous ones, but I was still expecting better scores. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Issues and posible extensions\n",
    "\n",
    "#### For the first part no major issues. In the second part I haven't been able to use Tensorflow recommenders because every time I tried to import it the kernel crashed. I had a similar issue in a previous homework but this time I didn't managed to solve it. For this reason I didn't follow the suggestion in the task description and I implemented a recommender based on sklearn k-nearest neighbors. A possible extension could be modifying the second part in order for the knn model to be able to compute the distance between users focusing not only on the rating they gave to movies but also the genre infromation, that isn't used in my implementation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
